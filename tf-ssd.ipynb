{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81ca3e50",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c6c8525",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "a = tf.constant([[1, 2], [3, 4]])                 \n",
    "b = tf.add(a, 1)\n",
    "\n",
    "print(a.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17bde9e4",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from utils import bbox_utils, data_utils, drawing_utils, io_utils, train_utils, eval_utils\n",
    "from models.decoder import get_decoder_model, SSDDecoder\n",
    "from models.ssd_mobilenet_v2 import get_model, init_model\n",
    "from models.header import HeadWrapper\n",
    "\n",
    "batch_size = 32\n",
    "evaluate = True\n",
    "use_custom_images = True\n",
    "custom_image_path = \"A:/data/ssd/\"\n",
    "backbone = \"mobilenet_v2\"\n",
    "io_utils.is_valid_backbone(backbone)\n",
    "hyper_params = train_utils.get_hyper_params(backbone)\n",
    "test_data, info = data_utils.get_dataset(\"voc/2007\", \"test\")\n",
    "total_items = data_utils.get_total_item_size(info, \"test\")\n",
    "labels = data_utils.get_labels(info)\n",
    "labels = [\"bg\"] + labels\n",
    "hyper_params[\"total_labels\"] = len(labels)\n",
    "img_size = hyper_params[\"img_size\"]\n",
    "\n",
    "data_types = data_utils.get_data_types()\n",
    "data_shapes = data_utils.get_data_shapes()\n",
    "padding_values = data_utils.get_padding_values()\n",
    "\n",
    "img_paths = data_utils.get_custom_imgs(custom_image_path)\n",
    "total_items = len(img_paths)\n",
    "test_data = tf.data.Dataset.from_generator(lambda: data_utils.custom_data_generator(\n",
    "                                           img_paths, img_size, img_size), data_types, data_shapes)\n",
    "test_data = test_data.padded_batch(batch_size, padded_shapes=data_shapes, padding_values=padding_values)\n",
    "\n",
    "ssd_model = get_model(hyper_params)\n",
    "ssd_model_path = io_utils.get_model_path(backbone)\n",
    "ssd_model.load_weights(ssd_model_path)\n",
    "\n",
    "print(\"ssd_model_path: \", ssd_model_path)\n",
    "print(hyper_params)\n",
    "prior_boxes = bbox_utils.generate_prior_boxes(hyper_params[\"feature_map_shapes\"], hyper_params[\"aspect_ratios\"])\n",
    "model = get_decoder_model(ssd_model, prior_boxes, hyper_params)\n",
    "\n",
    "step_size = train_utils.get_step_size(total_items, batch_size)\n",
    "pred_bboxes, pred_labels, pred_scores = model.predict(test_data, steps=step_size, verbose=1)\n",
    "\n",
    "drawing_utils.draw_predictions(test_data, pred_bboxes, pred_labels, pred_scores, labels, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79b24621",
   "metadata": {},
   "outputs": [],
   "source": [
    "for (index, label) in enumerate(labels):\n",
    "    print(index, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e779dfcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model, Model\n",
    "import os\n",
    "sample_test_dir = \"A:\\\\python_ai_projects\\\\tool_tracking\\\\tf-ssd\\python_outputs\"\n",
    "\n",
    "if not os.path.exists(sample_test_dir):\n",
    "    os.makedirs(sample_test_dir)\n",
    "    \n",
    "def pred_funct(image_in, model_p):\n",
    "    image_expanded = np.expand_dims(image_in, axis = 0)\n",
    "    tf_tensor = tf.convert_to_tensor(image_expanded.astype(np.float32))\n",
    "    return model_p.predict(tf_tensor)\n",
    "\n",
    "def generate_model(model, check_index):\n",
    "    input_layer_data = model.layers[0]\n",
    "    output_layer_data = model.layers[check_index]\n",
    "    model1 = Model(input_layer_data.input, output_layer_data.output)\n",
    "    return model1\n",
    "\n",
    "def write_csv3d(output_data, file_name_sufix):  \n",
    "    path_to_saved = os.path.join(sample_test_dir, file_name_sufix +\"_python.csv\")\n",
    "    print(output_data.shape)\n",
    "    # Loop over the rows and columns of the 3D array\n",
    "    with open(path_to_saved, 'w') as file:\n",
    "        for l in range(output_data.shape[3]):\n",
    "            for k in range(output_data.shape[2]):\n",
    "                for j in range(output_data.shape[1]):\n",
    "                    for i in range(output_data.shape[0]):\n",
    "                        # Join the text data in each row with commas\n",
    "                        value = output_data[i, j, k, l]\n",
    "                        # Write the row data to the output file\n",
    "                        if i == output_data.shape[1] - 1:\n",
    "                            file.write(\"{:.4f}\".format(value))\n",
    "                        else:\n",
    "                            file.write(\"{:.4f},\".format(value))\n",
    "                    file.write(\"\\n\")\n",
    "\n",
    "def write_npy(output_data, file_name_sufix):\n",
    "    path_to_saved = os.path.join(sample_test_dir, file_name_sufix + \".npy\")\n",
    "    np.save(path_to_saved, output_data)\n",
    "\n",
    "def write_csv(output_data, file_name_sufix):  \n",
    "    path_to_saved = os.path.join(sample_test_dir, file_name_sufix +\"_n.csv\")\n",
    "    print(output_data.shape)\n",
    "    with open(path_to_saved, 'w') as file:\n",
    "        for k in range(output_data.shape[2]):\n",
    "            for j in range(output_data.shape[0]):\n",
    "                for i in range(output_data.shape[1]):\n",
    "                    # Join the text data in each row with commas\n",
    "                    value = output_data[j, i, k]\n",
    "                    # Write the row data to the output file\n",
    "                    if i == output_data.shape[1] - 1:\n",
    "                        file.write(\"{:.4f}\".format(value))\n",
    "                    else:\n",
    "                        file.write(\"{:.4f},\".format(value))\n",
    "                file.write(\"\\n\")\n",
    "def write_csv_2d(output_data, file_name_sufix):  \n",
    "    path_to_saved = os.path.join(sample_test_dir, file_name_sufix +\"_n.csv\")\n",
    "    print(output_data.shape)\n",
    "    with open(path_to_saved, 'w') as file:\n",
    "        for j in range(output_data.shape[0]):\n",
    "            for i in range(output_data.shape[1]):\n",
    "                # Join the text data in each row with commas\n",
    "                value = output_data[j, i]\n",
    "                # Write the row data to the output file\n",
    "                if i == output_data.shape[1] - 1:\n",
    "                    file.write(\"{:.4f}\".format(value))\n",
    "                else:\n",
    "                    file.write(\"{:.4f},\".format(value))\n",
    "            file.write(\"\\n\")\n",
    "def write_csv_1d(output_data, file_name_sufix):  \n",
    "    path_to_saved = os.path.join(sample_test_dir, file_name_sufix +\"_n.csv\")\n",
    "    print(output_data.shape)\n",
    "    with open(path_to_saved, 'w') as file:\n",
    "        for k in range(output_data.shape[0]):\n",
    "            value = output_data[k]\n",
    "            # Write the row data to the output file\n",
    "            file.write(\"{:.4f}\\n\".format(value))\n",
    "\n",
    "def runthis(mode1, img_data, print_input, layer_name):\n",
    "    final_output = pred_funct(img_data, model1).squeeze()\n",
    "    if print_input:\n",
    "        print(\"-----------------\")\n",
    "        for k in range(3):\n",
    "            for i in range(6):\n",
    "                for j in range(6):\n",
    "                    print(img_data[i, j, k], end=\", \")\n",
    "                print()\n",
    "            print(\"\\n\")\n",
    "        print(img_data.shape)\n",
    "    print(final_output.shape)\n",
    "    print(\"-----------------\")\n",
    "    expected_shape = len(final_output.shape)\n",
    "    if expected_shape == 1:\n",
    "        for j in range(min(7, final_output.shape[0])):\n",
    "            print(final_output[j], end=\", \")\n",
    "        write_csv_1d(final_output, layer_name)  \n",
    "    elif expected_shape == 2:\n",
    "        for j in range(min(7, final_output.shape[0])):\n",
    "            for l in range(min(7, final_output.shape[1])):\n",
    "                print(final_output[j,l], end=\", \")\n",
    "        write_csv_2d(final_output, layer_name)  \n",
    "    else:\n",
    "        for k in range(min(4, final_output.shape[2])): \n",
    "            for i in range(min(7, final_output.shape[0])):\n",
    "                for j in range(min(7, final_output.shape[1])):\n",
    "                    print(final_output[i, j, k], end=\", \")\n",
    "                print()\n",
    "            print(\"\\n\")\n",
    "        write_csv(final_output, layer_name)   \n",
    "        #write_npy(final_output, layer_name)     \n",
    "    print(\"-----------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1a5f605",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "#model.summary()\n",
    "from tensorflow.keras.applications.mobilenet_v2 import preprocess_input\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
    "import numpy as np\n",
    "w = 300\n",
    "h = 300\n",
    "image_file_path = os.path.join(\"A:\\\\data\\\\ssd\\\\input.bmp\")\n",
    "selected_image = img_to_array(load_img(image_file_path).resize((w,h))).astype(np.uint8)\n",
    "\n",
    "img_data = (selected_image / 128.0) - 1.0\n",
    "print(sample_test_dir)\n",
    "#numpy_in = os.path.join(sample_test_dir, \"in_ssd.npy\")\n",
    "#np.save(numpy_in, np.transpose(img_data.copy().astype(np.float32),[1,0,2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad28d0fc",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "check_index = 1\n",
    "check_list1 = [\"1_conv_label_output\", \"2_conv_label_output\", \"3_conv_label_output\", \"4_conv_label_output\", \"5_conv_label_output\", \"6_conv_label_output\"]\n",
    "check_list2 = [\"1_conv_boxes_output\", \"2_conv_boxes_output\", \"3_conv_boxes_output\", \"4_conv_boxes_output\", \"5_conv_boxes_output\", \"6_conv_boxes_output\"]\n",
    "check_list = check_list1 + check_list2\n",
    "print_input = True\n",
    "for index, layer in enumerate(model.layers):\n",
    "    #print(index, layer.name)\n",
    "    if layer.name in [\"labels_head\", \"loc\"]:\n",
    "        print(layer.name)\n",
    "        model1 = generate_model(model, index)\n",
    "        runthis(model1, img_data, print_input, layer.name)\n",
    "        print_input = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e7b41ae",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from tensorflow.keras.layers import Activation, Concatenate, Add, ZeroPadding2D, BatchNormalization, Conv2D, Input, MaxPooling2D, Dropout, concatenate, UpSampling2D, ReLU, InputLayer, Conv2DTranspose, DepthwiseConv2D\n",
    "\n",
    "model_dir = \"A:/models/ssd/\"\n",
    "model_name = \"ssd\"\n",
    "\n",
    "epsilon = np.finfo(float).eps\n",
    "lines_in_graph = []\n",
    "\n",
    "model_data_dir = os.path.join(model_dir, model_name)\n",
    "if not os.path.exists(model_data_dir):\n",
    "    os.makedirs(model_data_dir)\n",
    "    \n",
    "for i, layer in enumerate(model.layers): \n",
    "    print(i, \":\", layer.__class__.__name__, layer.name)\n",
    "    items_to_write = [layer.__class__.__name__, layer.name]\n",
    "    if isinstance(layer,InputLayer):\n",
    "        inpu_shape = layer.input.shape\n",
    "        items_to_write.append(\"\")\n",
    "        items_to_write.append(str(inpu_shape[2]))\n",
    "        items_to_write.append(str(inpu_shape[1]))\n",
    "        items_to_write.append(str(inpu_shape[3]))\n",
    "    elif isinstance(layer.input, list):\n",
    "        input_layer_names = []\n",
    "        for i in range(len(layer.input)):\n",
    "            inpu_layer_name = layer.input[i].name.split('/')[0]\n",
    "            input_layer_names.append(inpu_layer_name)\n",
    "        items_to_write.append(\"&\".join(input_layer_names))\n",
    "        if isinstance(layer,Add):\n",
    "            for i in range(len(layer.input)):\n",
    "                inpu_shape = layer.input[i].shape\n",
    "                items_to_write.append(str(inpu_shape[2]))\n",
    "                items_to_write.append(str(inpu_shape[1]))\n",
    "                items_to_write.append(str(inpu_shape[3]))\n",
    "        elif isinstance(layer,Concatenate): \n",
    "            for i in range(len(layer.input)):\n",
    "                inpu_shape = layer.input[i].shape\n",
    "                items_to_write.append(str(inpu_shape[2]))\n",
    "                items_to_write.append(str(inpu_shape[1]))\n",
    "                items_to_write.append(str(inpu_shape[3]))\n",
    "        elif isinstance(layer,HeadWrapper):\n",
    "            for i in range(len(layer.input)):\n",
    "                inpu_shape = layer.input[i].shape\n",
    "                items_to_write.append(str(inpu_shape[2]))\n",
    "                items_to_write.append(str(inpu_shape[1]))\n",
    "                items_to_write.append(str(inpu_shape[3]))\n",
    "        else:\n",
    "            print(\"---------------------------------\")\n",
    "            print(\"Error: Multi input layer not known\")\n",
    "            print(\"---------------------------------\")            \n",
    "    else:\n",
    "        inpu_layer_name = layer.input.name.split('/')[0]\n",
    "        items_to_write.append(inpu_layer_name)\n",
    "        if isinstance(layer,Conv2D):\n",
    "            weights = layer.weights[0]\n",
    "            path_to_save = os.path.join(model_data_dir, layer.name + '_weights.npy')\n",
    "            np.save(path_to_save, weights)             \n",
    "            if layer.use_bias:\n",
    "                bias = layer.weights[1]\n",
    "                path_to_save = os.path.join(model_data_dir, layer.name + '_bias.npy')\n",
    "                np.save(path_to_save, bias)\n",
    "            layer_activation = layer.activation\n",
    "            if layer_activation:\n",
    "                items_to_write.append(layer_activation.__name__)\n",
    "            else:\n",
    "                print(\"Warning: Activation layer present : \", layer_activation.__name__)                \n",
    "                print(\"---------------------------------\")\n",
    "            weights_shape = layer.weights[0].shape\n",
    "            items_to_write.append(str(weights_shape[0]))\n",
    "            items_to_write.append(str(weights_shape[1]))\n",
    "            items_to_write.append(str(weights_shape[2]))\n",
    "            items_to_write.append(str(weights_shape[3]))\n",
    "            inpu_shape = layer.input.shape\n",
    "            items_to_write.append(str(inpu_shape[2]))\n",
    "            items_to_write.append(str(inpu_shape[1]))\n",
    "            items_to_write.append(str(inpu_shape[3]))\n",
    "            strides = layer.strides\n",
    "            if strides[0] != strides[1]:\n",
    "                print(\"---------------------------------\")\n",
    "                print(\"Error: Strides must be equal\")\n",
    "                print(\"---------------------------------\")\n",
    "            dilation_rates = layer.dilation_rate\n",
    "            if dilation_rates[0] != dilation_rates[1] and dilation_rates[1] != 1:\n",
    "                print(\"---------------------------------\")\n",
    "                print(\"Error: dilation_rates must be equal\")\n",
    "                print(\"---------------------------------\")                \n",
    "            items_to_write.append(str(strides[0]))\n",
    "            items_to_write.append(str(layer.use_bias))\n",
    "            items_to_write.append(str(layer.padding))            \n",
    "        elif isinstance(layer,Conv2DTranspose):\n",
    "            weights = layer.weights[0]\n",
    "            path_to_save = os.path.join(model_data_dir, layer.name + '_weights.npy')\n",
    "            np.save(path_to_save, weights)             \n",
    "            if layer.use_bias:\n",
    "                bias = layer.weights[1]\n",
    "                path_to_save = os.path.join(model_data_dir, layer.name + '_bias.npy')\n",
    "                np.save(path_to_save, bias)\n",
    "            weights_shape = layer.weights[0].shape\n",
    "            items_to_write.append(str(weights_shape[0]))\n",
    "            items_to_write.append(str(weights_shape[1]))\n",
    "            items_to_write.append(str(weights_shape[2]))\n",
    "            items_to_write.append(str(weights_shape[3]))\n",
    "            inpu_shape = layer.input.shape\n",
    "            items_to_write.append(str(inpu_shape[2]))\n",
    "            items_to_write.append(str(inpu_shape[1]))\n",
    "            items_to_write.append(str(inpu_shape[3]))\n",
    "            strides = layer.strides\n",
    "            if strides[0] != strides[1]:\n",
    "                print(\"---------------------------------\")\n",
    "                print(\"Error: Strides must be equal\")\n",
    "                print(\"---------------------------------\")\n",
    "            items_to_write.append(str(strides[0]))\n",
    "            items_to_write.append(str(layer.use_bias))\n",
    "        elif isinstance(layer,DepthwiseConv2D):   \n",
    "            weights = layer.weights[0]\n",
    "            path_to_save = os.path.join(model_data_dir, layer.name + '_weights.npy')\n",
    "            np.save(path_to_save, weights)             \n",
    "            if layer.use_bias:\n",
    "                bias = layer.weights[1]\n",
    "                path_to_save = os.path.join(model_data_dir, layer.name + '_bias.npy')\n",
    "                np.save(path_to_save, bias)\n",
    "            weights_shape = layer.weights[0].shape\n",
    "            items_to_write.append(str(weights_shape[0]))\n",
    "            items_to_write.append(str(weights_shape[1]))\n",
    "            items_to_write.append(str(weights_shape[2]))\n",
    "            inpu_shape = layer.input.shape\n",
    "            items_to_write.append(str(inpu_shape[2]))\n",
    "            items_to_write.append(str(inpu_shape[1]))\n",
    "            items_to_write.append(str(inpu_shape[3]))\n",
    "            strides = layer.strides\n",
    "            if strides[0] != strides[1]:\n",
    "                print(\"---------------------------------\")\n",
    "                print(\"Error: Strides must be equal\")\n",
    "                print(\"---------------------------------\")\n",
    "            items_to_write.append(str(strides[0]))\n",
    "            items_to_write.append(str(layer.use_bias))\n",
    "        elif isinstance(layer,BatchNormalization):\n",
    "            weights = layer.get_weights()\n",
    "            gamma = weights[0]\n",
    "            beta = weights[1]\n",
    "            moving_mean = weights[2]\n",
    "            moving_variance = weights[3]\n",
    "            a = gamma / np.sqrt(moving_variance + epsilon)\n",
    "            b = - moving_mean * a + beta\n",
    "            path_to_save = os.path.join(model_data_dir, layer.name + '_mean.npy')\n",
    "            np.save(path_to_save, b)\n",
    "            path_to_save = os.path.join(model_data_dir, layer.name + '_variance.npy')\n",
    "            np.save(path_to_save, a) \n",
    "\n",
    "            inpu_shape = layer.input.shape\n",
    "            items_to_write.append(str(inpu_shape[2]))\n",
    "            items_to_write.append(str(inpu_shape[1]))\n",
    "            items_to_write.append(str(inpu_shape[3]))\n",
    "        elif isinstance(layer,MaxPooling2D):\n",
    "            inpu_shape = layer.input.shape\n",
    "            items_to_write.append(str(inpu_shape[2]))\n",
    "            items_to_write.append(str(inpu_shape[1]))\n",
    "            items_to_write.append(str(inpu_shape[3]))\n",
    "        elif isinstance(layer,ZeroPadding2D):\n",
    "            if(layer.padding[0][1] != layer.padding[1][1] and layer.padding[0][0] == 0 and layer.padding[1][0] != 0):\n",
    "                print(layer.padding)\n",
    "                print(\"---------------------------------\")\n",
    "                print(\"Error: This padding not supported\") \n",
    "                print(\"---------------------------------\")\n",
    "            inpu_shape = layer.input.shape\n",
    "            items_to_write.append(str(inpu_shape[2]))\n",
    "            items_to_write.append(str(inpu_shape[1]))\n",
    "            items_to_write.append(str(inpu_shape[3]))\n",
    "            items_to_write.append(str(layer.padding[0][0]))\n",
    "            items_to_write.append(str(layer.padding[0][1]))\n",
    "            items_to_write.append(str(layer.padding[1][0]))\n",
    "            items_to_write.append(str(layer.padding[1][1]))\n",
    "        elif isinstance(layer, ReLU):\n",
    "            maxValue = layer.max_value\n",
    "            inpu_shape = layer.input.shape\n",
    "            items_to_write.append(str(inpu_shape[2]))\n",
    "            items_to_write.append(str(inpu_shape[1]))\n",
    "            items_to_write.append(str(inpu_shape[3]))\n",
    "            items_to_write.append(str(maxValue))\n",
    "        elif isinstance(layer, Activation):\n",
    "            activation_type = layer.get_config()['activation']\n",
    "            maxValue = 0\n",
    "            if activation_type == 'relu':\n",
    "                items_to_write.append(\"relu\")\n",
    "                maxValue = 0\n",
    "                if hasattr(layer,'max_value'):\n",
    "                    maxValue = layer.max_value\n",
    "                inpu_shape = layer.input.shape\n",
    "                items_to_write.append(str(inpu_shape[2]))\n",
    "                items_to_write.append(str(inpu_shape[1]))\n",
    "                items_to_write.append(str(inpu_shape[3]))\n",
    "                items_to_write.append(str(maxValue))\n",
    "            elif activation_type == 'sigmoid':\n",
    "                items_to_write.append(\"sigmoid\")\n",
    "                inpu_shape = layer.input.shape\n",
    "                items_to_write.append(str(inpu_shape[2]))\n",
    "                items_to_write.append(str(inpu_shape[1]))\n",
    "                items_to_write.append(str(inpu_shape[3]))\n",
    "            elif activation_type == 'softmax':\n",
    "                items_to_write.append(\"softmax\")\n",
    "                inpu_shape = layer.input.shape\n",
    "                output_shape = layer.output.shape\n",
    "                print(output_shape)\n",
    "                items_to_write.append(str(inpu_shape[1]))\n",
    "                items_to_write.append(str(inpu_shape[2]))\n",
    "            else:\n",
    "                print(\"---------------------------------\")\n",
    "                print(\"Error: Only Relu activation with max value supported, provided is \", activation_type)\n",
    "                print(\"---------------------------------\") \n",
    "        else:\n",
    "            print(\"Error: Layer is not identified \", layer.__class__.__name__, layer.name)\n",
    "    lines_in_graph.append(\",\".join(items_to_write))\n",
    "with open(f'{model_name}.txt', 'w') as f:\n",
    "    # Write each line of the list to the file\n",
    "    for line in lines_in_graph:            \n",
    "        f.write(line + '\\n')\n",
    "    print(\"File is saved to \" + f'{model_name}.txt')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b986bd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_bboxes, pred_labels, pred_scores = pred_funct(img_data, model)\n",
    "print(pred_bboxes.shape)\n",
    "print(pred_labels.shape)\n",
    "print(pred_scores.shape)\n",
    "drawing_utils.draw_predictions(test_data, pred_bboxes, pred_labels, pred_scores, labels, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ab30722",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(precision=5, suppress=True)\n",
    "filer_name = \"extra1_1\"\n",
    "path_to_saved = os.path.join(model_data_dir, f'{filer_name}_mean.npy')\n",
    "if os.path.exists(path_to_saved):\n",
    "    data = np.load(path_to_saved)\n",
    "    for i in range(min(5, data.size)):\n",
    "        print(data[i], end=\", \")\n",
    "    print()\n",
    "    print(data.shape)\n",
    "    print(\"\\n--------------\")\n",
    "    path_to_saved = os.path.join(model_data_dir, f'{filer_name}_variance.npy')\n",
    "    data = np.load(path_to_saved)\n",
    "    for i in range(min(5, data.size)):\n",
    "        print(data[i], end=\", \")\n",
    "    print()\n",
    "    print(data.shape)\n",
    "    print(\"\\n--------------\")\n",
    "else:\n",
    "    path_to_saved = os.path.join(model_data_dir, f'{filer_name}_bias.npy')\n",
    "    data = np.load(path_to_saved)\n",
    "    for i in range(min(5, data.size)):\n",
    "        print(data[i], end=\", \")\n",
    "    print()\n",
    "    print(data.shape)\n",
    "    print(\"\\n--------------\")\n",
    "    \n",
    "path_to_saved = os.path.join(model_data_dir, f'{filer_name}_weights.npy')\n",
    "if os.path.exists(path_to_saved):\n",
    "    data = np.load(path_to_saved)\n",
    "    if 'depth' in filer_name:\n",
    "        for i in range(3):\n",
    "            for j in range(3):\n",
    "                print(data[i, j, 0, 0], end=\", \")\n",
    "            print()\n",
    "        print()\n",
    "        for i in range(3):\n",
    "            for j in range(3):\n",
    "                print(data[i, j, 1, 0], end=\", \")\n",
    "            print()\n",
    "    elif data.shape[0] == data.shape[1] and data.shape[0] == 1 and len(data.shape) == 4:\n",
    "        print(data.shape)\n",
    "        for i in range(min(7, data.shape[3])):\n",
    "            for j in range(min(7, data.shape[2])):     \n",
    "                print(data[0,0,j,i], end=\", \")   \n",
    "            print(\"\\n\")\n",
    "    else:\n",
    "        for l in range(2):\n",
    "            for k in range(4):\n",
    "                for i in range(3):\n",
    "                    for j in range(3):\n",
    "                        print(data[j, i, k ,l], end=\", \")\n",
    "                    print()\n",
    "                print(\"\")\n",
    "            print(\"+++++++++\")\n",
    "        print(\"\\n--------------\")\n",
    "    print(data.shape)\n",
    "    print(\"\\n--------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52fa8d7d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
